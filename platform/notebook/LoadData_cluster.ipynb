{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions as F\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reads the YAML config file\n",
    "def read_yaml_config():\n",
    "    global cfg\n",
    "    with open(\"configuration.yml\", 'r') as ymlfile:\n",
    "        cfg = yaml.load(ymlfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get Amazon Access Key Id\n",
    "def get_amazon_access_key_id():\n",
    "    if 'cfg' not in globals():\n",
    "        read_yaml_config()\n",
    "    return cfg[\"amazon\"][\"access_key_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get Amazon Access Key Id\n",
    "def get_amazon_secret_access_key():\n",
    "    if 'cfg' not in globals():\n",
    "        read_yaml_config()\n",
    "    return cfg[\"amazon\"][\"secret_access_key\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "KEYSPACE = \"sparkassandra\"\n",
    "APP_NAME = KEYSPACE\n",
    "CASSANDRA_IP = \"cassandra_node\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SUBMIT_ARGS = \"--packages com.databricks:spark-csv_2.10:1.2.0,com.amazonaws:aws-java-sdk:1.10.34,org.apache.hadoop:hadoop-aws:2.6.0,com.datastax.spark:spark-cassandra-connector_2.10:1.6.0-M1 pyspark-shell\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = SUBMIT_ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make sure the driver and workers all use Python2\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/conda/envs/python2/bin/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f880944d790>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Init Spark Conf\n",
    "conf = pyspark.SparkConf().setAppName(APP_NAME)\n",
    "conf.set(\"spark.cassandra.connection.host\",CASSANDRA_IP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Init Spark Context\n",
    "sc = pyspark.SparkContext('spark://10.0.0.2:7077', conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Init Spark SQL Context\n",
    "sql_ctx = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hadoopConf = sc._jsc.hadoopConfiguration()\n",
    "hadoopConf.set(\"fs.s3n.impl\", \"org.apache.hadoop.fs.s3native.NativeS3FileSystem\")\n",
    "\n",
    "hadoopConf.set(\"fs.s3n.awsAccessKeyId\", get_amazon_access_key_id())\n",
    "hadoopConf.set(\"fs.s3n.awsSecretAccessKey\", get_amazon_secret_access_key())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Cassandra Keyspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/dkoepke/cassandra-python-driver"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "create keyspace IF NOT EXISTS sparkassandra WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Table us_flights"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CREATE TABLE sparkassandra.us_flights (\n",
    "  Id                text PRIMARY KEY,\n",
    "  Year              text,\n",
    "  Month             text,\n",
    "  DayOfMonth       text,\n",
    "  DayOfWeek         text,\n",
    "  DepTime           text,\n",
    "  CRSDepTime        text,\n",
    "  ArrTime           text,\n",
    "  CRSArrTime        text,\n",
    "  UniqueCarrier     text,\n",
    "  FlightNum         text,\n",
    "  TailNum           text,\n",
    "  ActualElapsedTime text,\n",
    "  CRSElapsedTime    text,\n",
    "  AirTime           text,\n",
    "  ArrDelay          text,\n",
    "  DepDelay          text,\n",
    "  Origin            text,\n",
    "  Dest              text,\n",
    "  Distance          text,\n",
    "  TaxiIn            text,\n",
    "  TaxiOut           text,\n",
    "  Cancelled         text,\n",
    "  CancellationCode  text,\n",
    "  Diverted          text,\n",
    "  CarrierDelay      text,\n",
    "  WeatherDelay      text,\n",
    "  NASDelay          text,\n",
    "  SecurityDelay     text,\n",
    "  LateAircraftDelay text\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Table us_airports"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CREATE TABLE sparkassandra.airports (\n",
    "  iata              text PRIMARY KEY,\n",
    "  airport           text,\n",
    "  city              text,\n",
    "  state             text,\n",
    "  country           text,\n",
    "  lat               text,\n",
    "  long              text\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Airport file and store in Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "airportRDD = sql_ctx.read.format('com.databricks.spark.csv').options(header='true').load('s3n://sparkassandra/airports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- iata: string (nullable = true)\n",
      " |-- airport: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- lat: string (nullable = true)\n",
      " |-- long: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airportRDD.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "airportRDD.write\\\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .mode('append')\\\n",
    "    .options(table=\"airports\", keyspace=KEYSPACE)\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load US Flights files and store in Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flightsRDD = sql_ctx.read.format('com.databricks.spark.csv').options(header='true').load('s3n://sparkassandra/1987.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add Id\n",
    "flightsRDD = flightsRDD.withColumn(\"Id\", F.concat_ws(\"-\", flightsRDD[\"Year\"], flightsRDD[\"Month\"], flightsRDD[\"DayofMonth\"], flightsRDD[\"DepTime\"], flightsRDD[\"FlightNum\"]));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for column in flightsRDD.columns:\n",
    "    flightsRDD = flightsRDD.withColumnRenamed(column, column.lower());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flightsRDD.registerTempTable(\"us_flights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- dayofmonth: string (nullable = true)\n",
      " |-- dayofweek: string (nullable = true)\n",
      " |-- deptime: string (nullable = true)\n",
      " |-- crsdeptime: string (nullable = true)\n",
      " |-- arrtime: string (nullable = true)\n",
      " |-- crsarrtime: string (nullable = true)\n",
      " |-- uniquecarrier: string (nullable = true)\n",
      " |-- flightnum: string (nullable = true)\n",
      " |-- tailnum: string (nullable = true)\n",
      " |-- actualelapsedtime: string (nullable = true)\n",
      " |-- crselapsedtime: string (nullable = true)\n",
      " |-- airtime: string (nullable = true)\n",
      " |-- arrdelay: string (nullable = true)\n",
      " |-- depdelay: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- distance: string (nullable = true)\n",
      " |-- taxiin: string (nullable = true)\n",
      " |-- taxiout: string (nullable = true)\n",
      " |-- cancelled: string (nullable = true)\n",
      " |-- cancellationcode: string (nullable = true)\n",
      " |-- diverted: string (nullable = true)\n",
      " |-- carrierdelay: string (nullable = true)\n",
      " |-- weatherdelay: string (nullable = true)\n",
      " |-- nasdelay: string (nullable = true)\n",
      " |-- securitydelay: string (nullable = true)\n",
      " |-- lateaircraftdelay: string (nullable = true)\n",
      " |-- id: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsRDD.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o80.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 6, platform_spark_worker_3.platform_spark_cluster_nw): java.io.InvalidClassException: org.apache.spark.sql.catalyst.expressions.Literal; local class incompatible: stream classdesc serialVersionUID = -4259705229845269663, local class serialVersionUID = 3305180847846277455\n\tat java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:616)\n\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1630)\n\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1521)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1781)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:1714)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1347)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:1714)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1347)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:373)\n\tat scala.collection.immutable.$colon$colon.readObject(List.scala:362)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1909)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:373)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1922)\n\tat com.datastax.spark.connector.RDDFunctions.saveToCassandra(RDDFunctions.scala:37)\n\tat org.apache.spark.sql.cassandra.CassandraSourceRelation.insert(CassandraSourceRelation.scala:78)\n\tat org.apache.spark.sql.cassandra.DefaultSource.createRelation(DefaultSource.scala:87)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:222)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:148)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.InvalidClassException: org.apache.spark.sql.catalyst.expressions.Literal; local class incompatible: stream classdesc serialVersionUID = -4259705229845269663, local class serialVersionUID = 3305180847846277455\n\tat java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:616)\n\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1630)\n\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1521)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1781)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:1714)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1347)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:1714)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1347)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:373)\n\tat scala.collection.immutable.$colon$colon.readObject(List.scala:362)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1909)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:373)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-a4a05b45431d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mflightsRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m    \u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"org.apache.spark.sql.cassandra\"\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'append'\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"us_flights\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeyspace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mKEYSPACE\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m    393\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    396\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/lib/py4j-0.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    811\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 813\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    814\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    815\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/spark/python/lib/py4j-0.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    306\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    307\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    309\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o80.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2.0 (TID 6, platform_spark_worker_3.platform_spark_cluster_nw): java.io.InvalidClassException: org.apache.spark.sql.catalyst.expressions.Literal; local class incompatible: stream classdesc serialVersionUID = -4259705229845269663, local class serialVersionUID = 3305180847846277455\n\tat java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:616)\n\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1630)\n\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1521)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1781)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:1714)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1347)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:1714)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1347)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:373)\n\tat scala.collection.immutable.$colon$colon.readObject(List.scala:362)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1909)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:373)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1922)\n\tat com.datastax.spark.connector.RDDFunctions.saveToCassandra(RDDFunctions.scala:37)\n\tat org.apache.spark.sql.cassandra.CassandraSourceRelation.insert(CassandraSourceRelation.scala:78)\n\tat org.apache.spark.sql.cassandra.DefaultSource.createRelation(DefaultSource.scala:87)\n\tat org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:222)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:148)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:381)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:209)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.InvalidClassException: org.apache.spark.sql.catalyst.expressions.Literal; local class incompatible: stream classdesc serialVersionUID = -4259705229845269663, local class serialVersionUID = 3305180847846277455\n\tat java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:616)\n\tat java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1630)\n\tat java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1521)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1781)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:1714)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1347)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.readArray(ObjectInputStream.java:1714)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1347)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:373)\n\tat scala.collection.immutable.$colon$colon.readObject(List.scala:362)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1909)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2018)\n\tat java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1942)\n\tat java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1808)\n\tat java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)\n\tat java.io.ObjectInputStream.readObject(ObjectInputStream.java:373)\n\tat org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:76)\n\tat org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:115)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "flightsRDD.write\\\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .mode('append')\\\n",
    "    .options(table=\"us_flights\", keyspace=KEYSPACE)\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sql_ctx.read.format(\"org.apache.spark.sql.cassandra\").options(table=\"us_flights\", keyspace=KEYSPACE).load().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CREATE TABLE sparkassandra.bonhomme (\n",
    "  nom              text PRIMARY KEY,\n",
    "  prenom           text,\n",
    "  age              text\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#RES = sql_ctx.read.format('com.databricks.spark.csv').options(header='true', delimiter=';').load('s3n://sparkassandra/file1.csv')\n",
    "#RES.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#RES.write\\\n",
    "#    .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "#    .mode('append')\\\n",
    "#    .options(table=\"bonhomme\", keyspace=KEYSPACE)\\\n",
    "#    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
